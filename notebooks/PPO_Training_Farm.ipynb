{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš€ NeuroTrader PPO Training Farm\n",
                "\n",
                "**Purpose:** Train multiple PPO model variants on GPU for later use in Hybrid AI\n",
                "\n",
                "**Output:** Models saved to Google Drive â†’ Download to Desktop â†’ Ghost Replay\n",
                "\n",
                "---\n",
                "\n",
                "## âš ï¸ Before Running:\n",
                "1. Go to **Runtime â†’ Change runtime type â†’ GPU (T4)**\n",
                "2. Make sure you have Google Drive space (~500MB per run)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Setup Environment"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "# Install dependencies\n",
                "!pip install stable-baselines3[extra] gymnasium pandas pyarrow -q\n",
                "\n",
                "print(\"\\nâœ… Dependencies installed!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create output directory\n",
                "import os\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/NeuroTrader_Models'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"âœ… Output directory: {OUTPUT_DIR}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Clone NeuroTrader Repository"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Clone the repo (replace with your repo URL)\n",
                "!git clone -b NeuroTrader-Windows https://github.com/YOUR_USERNAME/NeuroTrader.git\n",
                "%cd NeuroTrader\n",
                "\n",
                "# Or upload data manually if repo is private\n",
                "print(\"\\nðŸ“ Current directory:\", os.getcwd())"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Upload Training Data (If not in repo)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Option 1: Upload from local machine\n",
                "from google.colab import files\n",
                "\n",
                "print(\"ðŸ“¤ Upload XAUUSDm_M5_raw.parquet file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Move to data folder\n",
                "import shutil\n",
                "os.makedirs('data/raw', exist_ok=True)\n",
                "for filename in uploaded.keys():\n",
                "    shutil.move(filename, f'data/raw/{filename}')\n",
                "    print(f\"âœ… Moved {filename} to data/raw/\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Define Training Environment"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import gymnasium as gym\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from gymnasium import spaces\n",
                "\n",
                "class SimpleTradingEnv(gym.Env):\n",
                "    \"\"\"\n",
                "    Simplified Trading Environment for PPO Farm.\n",
                "    Actions: 0=HOLD, 1=BUY, 2=SELL\n",
                "    \"\"\"\n",
                "    def __init__(self, df, lookback=20):\n",
                "        super().__init__()\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.lookback = lookback\n",
                "        self.current_step = lookback\n",
                "        \n",
                "        # Actions: HOLD, BUY, SELL\n",
                "        self.action_space = spaces.Discrete(3)\n",
                "        \n",
                "        # Observation: OHLCV + returns + position\n",
                "        self.observation_space = spaces.Box(\n",
                "            low=-np.inf, high=np.inf, shape=(22,), dtype=np.float32\n",
                "        )\n",
                "        \n",
                "        # State\n",
                "        self.position = 0  # 0=flat, 1=long\n",
                "        self.entry_price = 0\n",
                "        self.balance = 10000\n",
                "        self.equity = 10000\n",
                "        \n",
                "    def reset(self, seed=None, options=None):\n",
                "        super().reset(seed=seed)\n",
                "        self.current_step = self.lookback\n",
                "        self.position = 0\n",
                "        self.entry_price = 0\n",
                "        self.balance = 10000\n",
                "        self.equity = 10000\n",
                "        return self._get_obs(), {}\n",
                "    \n",
                "    def _get_obs(self):\n",
                "        window = self.df.iloc[self.current_step-self.lookback:self.current_step]\n",
                "        \n",
                "        # Compute features\n",
                "        close = window['close'].values\n",
                "        returns = np.diff(close) / close[:-1]\n",
                "        \n",
                "        # Normalize\n",
                "        obs = np.concatenate([\n",
                "            returns[-10:],  # Last 10 returns\n",
                "            [close[-1] / close[0] - 1],  # Period return\n",
                "            [(close[-1] - close.min()) / (close.max() - close.min() + 1e-8)],  # Position in range\n",
                "            [np.std(returns)],  # Volatility\n",
                "            [self.position],  # Current position\n",
                "            [self.equity / 10000 - 1],  # PnL %\n",
                "            np.zeros(7)  # Padding\n",
                "        ])\n",
                "        return obs.astype(np.float32)\n",
                "    \n",
                "    def step(self, action):\n",
                "        current_price = self.df.iloc[self.current_step]['close']\n",
                "        reward = 0\n",
                "        \n",
                "        # Execute action\n",
                "        if action == 1 and self.position == 0:  # BUY\n",
                "            self.position = 1\n",
                "            self.entry_price = current_price\n",
                "            reward = 0.01  # Small entry bonus\n",
                "            \n",
                "        elif action == 2 and self.position == 1:  # SELL\n",
                "            pnl_pct = (current_price - self.entry_price) / self.entry_price\n",
                "            self.balance *= (1 + pnl_pct)\n",
                "            reward = pnl_pct * 100  # Scale for learning\n",
                "            self.position = 0\n",
                "            self.entry_price = 0\n",
                "        \n",
                "        # Update equity\n",
                "        if self.position == 1:\n",
                "            unrealized = (current_price - self.entry_price) / self.entry_price\n",
                "            self.equity = self.balance * (1 + unrealized)\n",
                "        else:\n",
                "            self.equity = self.balance\n",
                "        \n",
                "        # Move forward\n",
                "        self.current_step += 1\n",
                "        done = self.current_step >= len(self.df) - 1\n",
                "        truncated = self.equity < 5000  # Circuit breaker\n",
                "        \n",
                "        return self._get_obs(), reward, done, truncated, {'equity': self.equity}\n",
                "\n",
                "print(\"âœ… Trading Environment defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. PPO Training Farm"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.callbacks import CheckpointCallback\n",
                "import torch\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "# Load data\n",
                "DATA_PATH = 'data/raw/XAUUSDm_M5_raw.parquet'\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "if 'tick_volume' in df.columns:\n",
                "    df.rename(columns={'tick_volume': 'volume'}, inplace=True)\n",
                "print(f\"âœ… Loaded {len(df)} rows from {DATA_PATH}\")\n",
                "\n",
                "# Verify GPU\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"ðŸ–¥ï¸ Training on: {device.upper()}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Training Configuration\n",
                "VARIANTS = {\n",
                "    'aggressive': {'ent_coef': 0.05, 'learning_rate': 5e-4, 'n_steps': 1024},\n",
                "    'balanced':   {'ent_coef': 0.02, 'learning_rate': 3e-4, 'n_steps': 2048},\n",
                "    'conservative': {'ent_coef': 0.01, 'learning_rate': 1e-4, 'n_steps': 4096},\n",
                "}\n",
                "\n",
                "TOTAL_TIMESTEPS = 500_000  # Per variant\n",
                "SEEDS = [42, 123, 456]  # Multiple seeds for diversity\n",
                "\n",
                "print(f\"ðŸ“‹ Training Plan:\")\n",
                "print(f\"   Variants: {list(VARIANTS.keys())}\")\n",
                "print(f\"   Seeds per variant: {len(SEEDS)}\")\n",
                "print(f\"   Total models: {len(VARIANTS) * len(SEEDS)}\")\n",
                "print(f\"   Steps per model: {TOTAL_TIMESTEPS:,}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Train all variants\n",
                "trained_models = []\n",
                "\n",
                "for variant_name, config in VARIANTS.items():\n",
                "    for seed in SEEDS:\n",
                "        model_name = f\"ppo_{variant_name}_seed{seed}\"\n",
                "        print(f\"\\n{'='*50}\")\n",
                "        print(f\"ðŸš€ Training: {model_name}\")\n",
                "        print(f\"   Config: {config}\")\n",
                "        \n",
                "        # Create env\n",
                "        env = SimpleTradingEnv(df)\n",
                "        \n",
                "        # Create model\n",
                "        model = PPO(\n",
                "            \"MlpPolicy\",\n",
                "            env,\n",
                "            device=device,\n",
                "            verbose=0,\n",
                "            seed=seed,\n",
                "            **config\n",
                "        )\n",
                "        \n",
                "        # Train\n",
                "        start_time = datetime.now()\n",
                "        model.learn(\n",
                "            total_timesteps=TOTAL_TIMESTEPS,\n",
                "            progress_bar=True\n",
                "        )\n",
                "        duration = (datetime.now() - start_time).total_seconds()\n",
                "        \n",
                "        # Save to Drive\n",
                "        save_path = f\"{OUTPUT_DIR}/{model_name}\"\n",
                "        model.save(save_path)\n",
                "        \n",
                "        # Record metadata\n",
                "        trained_models.append({\n",
                "            'name': model_name,\n",
                "            'path': f\"{save_path}.zip\",\n",
                "            'variant': variant_name,\n",
                "            'seed': seed,\n",
                "            'config': config,\n",
                "            'timesteps': TOTAL_TIMESTEPS,\n",
                "            'duration_sec': duration\n",
                "        })\n",
                "        \n",
                "        print(f\"âœ… Saved: {save_path}.zip ({duration:.1f}s)\")\n",
                "\n",
                "# Save manifest\n",
                "manifest_path = f\"{OUTPUT_DIR}/training_manifest.json\"\n",
                "with open(manifest_path, 'w') as f:\n",
                "    json.dump(trained_models, f, indent=2)\n",
                "\n",
                "print(f\"\\n{'='*50}\")\n",
                "print(f\"ðŸŽ‰ TRAINING COMPLETE!\")\n",
                "print(f\"   Models saved: {len(trained_models)}\")\n",
                "print(f\"   Manifest: {manifest_path}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Quick Evaluation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Test each model\n",
                "results = []\n",
                "\n",
                "for model_info in trained_models:\n",
                "    model = PPO.load(model_info['path'].replace('.zip', ''), device='cpu')\n",
                "    env = SimpleTradingEnv(df[-5000:])  # Test on last portion\n",
                "    \n",
                "    obs, _ = env.reset()\n",
                "    total_reward = 0\n",
                "    trades = 0\n",
                "    \n",
                "    for _ in range(4000):\n",
                "        action, _ = model.predict(obs, deterministic=True)\n",
                "        obs, reward, done, trunc, info = env.step(action)\n",
                "        total_reward += reward\n",
                "        if action in [1, 2]:\n",
                "            trades += 1\n",
                "        if done or trunc:\n",
                "            break\n",
                "    \n",
                "    final_return = (info['equity'] / 10000 - 1) * 100\n",
                "    results.append({\n",
                "        'model': model_info['name'],\n",
                "        'return_pct': final_return,\n",
                "        'trades': trades\n",
                "    })\n",
                "    print(f\"{model_info['name']}: {final_return:+.2f}% | {trades} trades\")\n",
                "\n",
                "# Save results\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df.to_csv(f\"{OUTPUT_DIR}/evaluation_results.csv\", index=False)\n",
                "print(f\"\\nðŸ“Š Results saved to {OUTPUT_DIR}/evaluation_results.csv\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7. Download Models\n",
                "\n",
                "à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ Google Drive: `MyDrive/NeuroTrader_Models/`\n",
                "\n",
                "**à¸§à¸´à¸˜à¸µà¸™à¸³à¹„à¸›à¹ƒà¸Šà¹‰:**\n",
                "1. Download à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ `NeuroTrader_Models` à¸ˆà¸²à¸ Google Drive\n",
                "2. Copy à¹„à¸›à¸¢à¸±à¸‡ Desktop: `NeuroTrader/models/farm/`\n",
                "3. à¸£à¸±à¸™ Ghost Replay: `python scripts/replay_ghosts.py --source models/farm/`"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# List all saved files\n",
                "print(\"ðŸ“ Files in output directory:\")\n",
                "for f in os.listdir(OUTPUT_DIR):\n",
                "    size = os.path.getsize(f\"{OUTPUT_DIR}/{f}\") / 1024 / 1024\n",
                "    print(f\"   {f} ({size:.2f} MB)\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}