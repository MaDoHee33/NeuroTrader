{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš€ NeuroTrader PPO Training Farm (Dynamic)\n",
                "\n",
                "**Purpose:** Train PPO models with **randomized hyperparameters** for Hybrid AI\n",
                "\n",
                "**Key Feature:** Each run generates unique configs automatically!\n",
                "\n",
                "---\n",
                "\n",
                "## âš ï¸ Before Running:\n",
                "1. Go to **Runtime â†’ Change runtime type â†’ GPU (T4)**\n",
                "2. Upload your data file when prompted"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Setup"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!nvidia-smi\n",
                "!pip install stable-baselines3[extra] gymnasium pandas pyarrow -q\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore', message='.*Gym has been unmaintained.*')\n",
                "\n",
                "print(\"\\nâœ… Setup complete!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/NeuroTrader_Models'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"âœ… Output: {OUTPUT_DIR}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Upload Data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "import shutil\n",
                "\n",
                "print(\"ðŸ“¤ Upload XAUUSDm_M5_raw.parquet:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "os.makedirs('data/raw', exist_ok=True)\n",
                "for f in uploaded.keys():\n",
                "    shutil.move(f, f'data/raw/{f}')\n",
                "    print(f\"âœ… Saved: data/raw/{f}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Dynamic Config Generator ðŸŽ²"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "import json\n",
                "import time\n",
                "from datetime import datetime\n",
                "\n",
                "def generate_random_config():\n",
                "    \"\"\"\n",
                "    Generate randomized hyperparameters for diverse model training.\n",
                "    Each call produces unique values!\n",
                "    \"\"\"\n",
                "    # Random seed based on current time (ensures uniqueness)\n",
                "    seed = int(time.time() * 1000) % 1000000\n",
                "    np.random.seed(seed)\n",
                "    \n",
                "    config = {\n",
                "        'seed': seed,\n",
                "        'learning_rate': np.random.choice([1e-4, 3e-4, 5e-4, 7e-4, 1e-3]),\n",
                "        'n_steps': np.random.choice([512, 1024, 2048, 4096]),\n",
                "        'batch_size': np.random.choice([32, 64, 128, 256]),\n",
                "        'ent_coef': np.random.uniform(0.005, 0.1),\n",
                "        'gamma': np.random.choice([0.95, 0.97, 0.99, 0.995]),\n",
                "        'gae_lambda': np.random.uniform(0.9, 0.99),\n",
                "        'clip_range': np.random.choice([0.1, 0.2, 0.3]),\n",
                "        'n_epochs': np.random.choice([5, 10, 15, 20]),\n",
                "        'max_grad_norm': np.random.choice([0.3, 0.5, 1.0]),\n",
                "    }\n",
                "    return config\n",
                "\n",
                "# Generate config for THIS run\n",
                "CONFIG = generate_random_config()\n",
                "RUN_ID = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
                "\n",
                "print(f\"ðŸŽ² Generated Config for {RUN_ID}:\")\n",
                "for k, v in CONFIG.items():\n",
                "    print(f\"   {k}: {v}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Trading Environment"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import gymnasium as gym\n",
                "import pandas as pd\n",
                "from gymnasium import spaces\n",
                "\n",
                "class SimpleTradingEnv(gym.Env):\n",
                "    def __init__(self, df, lookback=20):\n",
                "        super().__init__()\n",
                "        self.df = df.reset_index(drop=True)\n",
                "        self.lookback = lookback\n",
                "        self.current_step = lookback\n",
                "        self.action_space = spaces.Discrete(3)  # HOLD, BUY, SELL\n",
                "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(22,), dtype=np.float32)\n",
                "        self.position = 0\n",
                "        self.entry_price = 0\n",
                "        self.balance = 10000\n",
                "        self.equity = 10000\n",
                "        \n",
                "    def reset(self, seed=None, options=None):\n",
                "        super().reset(seed=seed)\n",
                "        self.current_step = self.lookback\n",
                "        self.position = 0\n",
                "        self.entry_price = 0\n",
                "        self.balance = 10000\n",
                "        self.equity = 10000\n",
                "        return self._get_obs(), {}\n",
                "    \n",
                "    def _get_obs(self):\n",
                "        window = self.df.iloc[self.current_step-self.lookback:self.current_step]\n",
                "        close = window['close'].values\n",
                "        returns = np.diff(close) / (close[:-1] + 1e-8)\n",
                "        obs = np.concatenate([\n",
                "            returns[-10:],\n",
                "            [close[-1] / close[0] - 1],\n",
                "            [(close[-1] - close.min()) / (close.max() - close.min() + 1e-8)],\n",
                "            [np.std(returns)],\n",
                "            [self.position],\n",
                "            [self.equity / 10000 - 1],\n",
                "            np.zeros(7)\n",
                "        ])\n",
                "        return obs.astype(np.float32)\n",
                "    \n",
                "    def step(self, action):\n",
                "        price = self.df.iloc[self.current_step]['close']\n",
                "        reward = 0\n",
                "        \n",
                "        if action == 1 and self.position == 0:\n",
                "            self.position = 1\n",
                "            self.entry_price = price\n",
                "            reward = 0.01\n",
                "        elif action == 2 and self.position == 1:\n",
                "            pnl = (price - self.entry_price) / self.entry_price\n",
                "            self.balance *= (1 + pnl)\n",
                "            reward = pnl * 100\n",
                "            self.position = 0\n",
                "        \n",
                "        if self.position == 1:\n",
                "            self.equity = self.balance * (1 + (price - self.entry_price) / self.entry_price)\n",
                "        else:\n",
                "            self.equity = self.balance\n",
                "        \n",
                "        self.current_step += 1\n",
                "        done = self.current_step >= len(self.df) - 1\n",
                "        truncated = self.equity < 5000\n",
                "        return self._get_obs(), reward, done, truncated, {'equity': self.equity}\n",
                "\n",
                "print(\"âœ… Environment ready!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. Train with Dynamic Config ðŸš€"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from stable_baselines3 import PPO\n",
                "import torch\n",
                "\n",
                "# Load data\n",
                "df = pd.read_parquet('data/raw/XAUUSDm_M5_raw.parquet')\n",
                "if 'tick_volume' in df.columns:\n",
                "    df.rename(columns={'tick_volume': 'volume'}, inplace=True)\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"ðŸ“Š Data: {len(df)} rows | Device: {device.upper()}\")\n",
                "\n",
                "# Create environment and model with DYNAMIC config\n",
                "env = SimpleTradingEnv(df)\n",
                "\n",
                "model = PPO(\n",
                "    \"MlpPolicy\",\n",
                "    env,\n",
                "    device=device,\n",
                "    verbose=1,\n",
                "    seed=CONFIG['seed'],\n",
                "    learning_rate=CONFIG['learning_rate'],\n",
                "    n_steps=int(CONFIG['n_steps']),\n",
                "    batch_size=int(CONFIG['batch_size']),\n",
                "    ent_coef=CONFIG['ent_coef'],\n",
                "    gamma=CONFIG['gamma'],\n",
                "    gae_lambda=CONFIG['gae_lambda'],\n",
                "    clip_range=CONFIG['clip_range'],\n",
                "    n_epochs=int(CONFIG['n_epochs']),\n",
                "    max_grad_norm=CONFIG['max_grad_norm']\n",
                ")\n",
                "\n",
                "print(f\"\\nðŸš€ Training {RUN_ID}...\")\n",
                "start = datetime.now()\n",
                "model.learn(total_timesteps=500_000, progress_bar=True)\n",
                "duration = (datetime.now() - start).total_seconds()\n",
                "\n",
                "# Save model\n",
                "save_path = f\"{OUTPUT_DIR}/ppo_{RUN_ID}\"\n",
                "model.save(save_path)\n",
                "print(f\"\\nâœ… Saved: {save_path}.zip ({duration/60:.1f} min)\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Evaluate & Log"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Quick evaluation\n",
                "env_test = SimpleTradingEnv(df[-5000:])\n",
                "obs, _ = env_test.reset()\n",
                "trades = 0\n",
                "\n",
                "for _ in range(4000):\n",
                "    action, _ = model.predict(obs, deterministic=True)\n",
                "    obs, _, done, trunc, info = env_test.step(action)\n",
                "    if action in [1, 2]: trades += 1\n",
                "    if done or trunc: break\n",
                "\n",
                "final_return = (info['equity'] / 10000 - 1) * 100\n",
                "print(f\"ðŸ“ˆ Result: {final_return:+.2f}% | {trades} trades\")\n",
                "\n",
                "# Save to training log\n",
                "log_path = f\"{OUTPUT_DIR}/training_log.json\"\n",
                "log_entry = {\n",
                "    'run_id': RUN_ID,\n",
                "    'config': {k: float(v) if isinstance(v, (np.floating, np.integer)) else v for k, v in CONFIG.items()},\n",
                "    'result': {'return_pct': final_return, 'trades': trades},\n",
                "    'duration_min': duration/60,\n",
                "    'timestamp': datetime.now().isoformat()\n",
                "}\n",
                "\n",
                "# Append to log\n",
                "try:\n",
                "    with open(log_path, 'r') as f:\n",
                "        logs = json.load(f)\n",
                "except:\n",
                "    logs = []\n",
                "logs.append(log_entry)\n",
                "with open(log_path, 'w') as f:\n",
                "    json.dump(logs, f, indent=2)\n",
                "\n",
                "print(f\"ðŸ“ Logged to: {log_path}\")\n",
                "print(f\"\\nðŸ”„ To train another model with NEW config: Runtime â†’ Run all\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7. View Training History"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Show all trained models\n",
                "try:\n",
                "    with open(log_path, 'r') as f:\n",
                "        logs = json.load(f)\n",
                "    \n",
                "    print(f\"ðŸ“Š Training History ({len(logs)} models):\")\n",
                "    print(\"-\" * 60)\n",
                "    for log in logs:\n",
                "        r = log['result']\n",
                "        print(f\"{log['run_id']}: {r['return_pct']:+.2f}% | {r['trades']} trades | LR={log['config']['learning_rate']:.0e}\")\n",
                "except:\n",
                "    print(\"No training history yet.\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}