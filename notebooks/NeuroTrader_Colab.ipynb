{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  NeuroTrader Ultimate: Cloud Training (Colab)\n",
                "\n",
                "Welcome to the high-performance training environment. This notebook allows you to train the NeuroTrader AI agent using Google Colab's powerful resources.\n",
                "\n",
                "### Steps:\n",
                "1. **Setup**: Install dependencies.\n",
                "2. **Connect Drive**: Mount Google Drive to save logs/models persistently.\n",
                "3. **Upload Data**: Upload your `.parquet` processed data.\n",
                "4. **Define Environment**: Initialize the `TradingEnv`.\n",
                "5. **Train**: Run the PPO algorithm for millions of timesteps.\n",
                "6. **Download**: Save the trained brain (`model.zip`) to use locally."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "**Note:** If you see errors about `numpy` or `gym`, try clicking **Runtime > Restart Session** and running this cell again."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies with version pinning to avoid Numpy 2.0 conflicts\n",
                "# --force-reinstall ensures we get binaries compiled against the OLD numpy, not the pre-installed new one\n",
                "!pip install \"numpy<2.0\" gymnasium==0.29.1 \"stable-baselines3[extra]>=2.3.0\" pandas ta pyarrow shimmy>=1.3.0 --force-reinstall"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.callbacks import EvaluatorCallback, CheckpointCallback\n",
                "from stable_baselines3.common.monitor import Monitor\n",
                "from typing import Optional\n",
                "import os\n",
                "import glob\n",
                "from google.colab import drive\n",
                "\n",
                "# Check GPU availability\n",
                "import torch\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Connect Google Drive â˜ï¸\n",
                "Connect to Drive to save logs and models automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "drive.mount('/content/drive')\n",
                "\n",
                "# Defines where to save logs\n",
                "# Usually maps to: /content/drive/MyDrive/...\n",
                "# You can change this path to match your folder ID if needed, but standard path is safest.\n",
                "LOG_DIR = \"/content/drive/MyDrive/NeuroTrader_Workspace/logs\"\n",
                "MODEL_DIR = \"/content/drive/MyDrive/NeuroTrader_Workspace/models\"\n",
                "\n",
                "os.makedirs(LOG_DIR, exist_ok=True)\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "print(f\"âœ… Logging to: {LOG_DIR}\")\n",
                "print(f\"âœ… Models will save to: {MODEL_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Upload Data\n",
                "Please upload your processed Parquet files (e.g. `BTCUSD_M1.parquet`) to the Colab runtime (drag and drop to the file panel on the left)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data (Dynamic Detection)\n",
                "parquet_files = glob.glob(\"*.parquet\")\n",
                "\n",
                "if parquet_files:\n",
                "    print(f\"ðŸ“‚ Found {len(parquet_files)} data files: {parquet_files}\")\n",
                "    \n",
                "    # Select the first one by default, or change index to select others\n",
                "    DATA_FILE = parquet_files[0]\n",
                "    print(f\"ðŸ‘‰ Training Target: {DATA_FILE}\")\n",
                "    \n",
                "    df = pd.read_parquet(DATA_FILE)\n",
                "    print(f\"âœ… Loaded Data: {len(df):,} rows\")\n",
                "    print(df.head())\n",
                "else:\n",
                "    print(\"âŒ No .parquet files found! Please upload your data to the Files panel on the left.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Define Trading Environment\n",
                "(Self-contained class to avoid file dependencies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TradingEnv(gym.Env):\n",
                "    metadata = {'render_modes': ['human']}\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, initial_balance=10000, max_steps=None):\n",
                "        super(TradingEnv, self).__init__()\n",
                "        self.df = df\n",
                "        self.initial_balance = initial_balance\n",
                "        self.max_steps = max_steps if max_steps else len(df) - 1\n",
                "        \n",
                "        # Actions: 0=HOLD, 1=BUY, 2=SELL\n",
                "        self.action_space = spaces.Discrete(3)\n",
                "        \n",
                "        # Features expected in DF\n",
                "        self.feature_cols = [\n",
                "            'close', 'rsi', 'macd', 'macd_signal', \n",
                "            'bb_high', 'bb_low', 'ema_20', 'ema_50'\n",
                "        ]\n",
                "        \n",
                "        # Check cols\n",
                "        missing = [c for c in self.feature_cols if c not in df.columns]\n",
                "        if missing: raise ValueError(f\"Missing cols: {missing}\")\n",
                "\n",
                "        # Obs: Features + Balance + Position\n",
                "        # Using Float32 for compatibility with PPO MlpPolicy\n",
                "        self.observation_space = spaces.Box(\n",
                "            low=-np.inf, high=np.inf, shape=(len(self.feature_cols) + 2,), dtype=np.float32\n",
                "        )\n",
                "        \n",
                "        self.current_step = 0\n",
                "        self.balance = initial_balance\n",
                "        self.position = 0.0\n",
                "        self.equity = initial_balance\n",
                "\n",
                "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
                "        super().reset(seed=seed)\n",
                "        self.current_step = 0\n",
                "        self.balance = self.initial_balance\n",
                "        self.position = 0.0\n",
                "        self.equity = self.initial_balance\n",
                "        return self._get_observation(), {}\n",
                "\n",
                "    def _get_observation(self):\n",
                "        row = self.df.iloc[self.current_step]\n",
                "        obs = [row[col] for col in self.feature_cols]\n",
                "        obs.append(self.balance)\n",
                "        obs.append(self.position)\n",
                "        return np.array(obs, dtype=np.float32)\n",
                "\n",
                "    def step(self, action):\n",
                "        current_price = self.df.iloc[self.current_step]['close']\n",
                "        prev_equity = self.equity\n",
                "        \n",
                "        # Logic: Fixed fractional or simple all-in for logic training\n",
                "        # Let's simple logical units: Buy = spend available / Sell = sell all\n",
                "        fee_rate = 0.001\n",
                "        \n",
                "        if action == 1 and self.balance > 0: # BUY\n",
                "            cost = self.balance\n",
                "            fee = cost * fee_rate\n",
                "            units = (cost - fee) / current_price\n",
                "            self.position += units\n",
                "            self.balance = 0\n",
                "            \n",
                "        elif action == 2 and self.position > 0: # SELL\n",
                "            revenue = self.position * current_price\n",
                "            fee = revenue * fee_rate\n",
                "            self.balance += (revenue - fee)\n",
                "            self.position = 0\n",
                "\n",
                "        self.current_step += 1\n",
                "        \n",
                "        # Equity calc\n",
                "        crypto_val = self.position * self.df.iloc[self.current_step]['close'] if self.current_step < len(self.df) else 0\n",
                "        self.equity = self.balance + crypto_val\n",
                "        \n",
                "        # Reward: Log Return\n",
                "        reward = 0\n",
                "        if prev_equity > 0:\n",
                "            reward = np.log(self.equity / prev_equity) * 100 # Scaling for stability\n",
                "            \n",
                "        done = self.current_step >= len(self.df) - 1\n",
                "        if self.equity < self.initial_balance * 0.1: # Bust (<10%)\n",
                "            done = True\n",
                "            reward = -100\n",
                "            \n",
                "        return self._get_observation(), reward, done, False, {'equity': self.equity}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train Agent ðŸ‹ï¸\n",
                "We use **PPO (Proximal Policy Optimization)**. It's stable and effective."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Env\n",
                "env = TradingEnv(df)\n",
                "env = Monitor(env) # For logging\n",
                "\n",
                "# Callbacks\n",
                "checkpoint_callback = CheckpointCallback(\n",
                "    save_freq=100000, # Save model every 100k steps\n",
                "    save_path=MODEL_DIR,\n",
                "    name_prefix=\"neurotrader_ppo\"\n",
                ")\n",
                "\n",
                "# Define Model\n",
                "model = PPO(\n",
                "    \"MlpPolicy\", \n",
                "    env, \n",
                "    verbose=1, \n",
                "    device=device,\n",
                "    tensorboard_log=LOG_DIR, # Save logs to Drive\n",
                "    learning_rate=0.0003,\n",
                "    n_steps=2048,\n",
                "    batch_size=64,\n",
                "    gamma=0.99\n",
                ")\n",
                "\n",
                "# Train\n",
                "print(\"ðŸš€ Training Started... (Logs are saved to your Drive)\")\n",
                "model.learn(\n",
                "    total_timesteps=3_000_000, # Increased for serious training\n",
                "    progress_bar=True,\n",
                "    callback=checkpoint_callback\n",
                ")\n",
                "print(\"âœ¨ Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Final Model ðŸ’¾\n",
                "The model is already saved in Drive via checkpoints, but we force a final save here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "final_path = os.path.join(MODEL_DIR, \"neurotrader_brain_final\")\n",
                "model.save(final_path)\n",
                "print(f\"Final Model saved to: {final_path}.zip\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualize (TensorBoard)\n",
                "Run this cell to see training graphs inside Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard\n",
                "%tensorboard --logdir \"$LOG_DIR\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}