# MCP Server for Ollama

MCP (Model Context Protocol) Server ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Ollama API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á

## ‚ö° Quick Start

### 1. Install Dependencies
```powershell
cd C:\Users\pp\.gemini\antigravity\scratch\mcp-ollama-server
pip install -r requirements.txt
```

### 2. Start Ollama Server
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà:
```powershell
ollama list
```

### 3. Run MCP Server
```powershell
python server.py
```

---

## üîß Configuration for Claude Desktop

‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô `%APPDATA%\Claude\claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "ollama": {
      "command": "python",
      "args": ["C:\\Users\\pp\\.gemini\\antigravity\\scratch\\mcp-ollama-server\\server.py"]
    }
  }
}
```

---

## üõ†Ô∏è Available Tools

| Tool | Description |
|------|-------------|
| `ask_deepseek` | ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á DeepSeek model |
| `ask_with_context` | ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏° context (code, document) |
| `list_ollama_models` | ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ |
| `check_ollama_status` | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Ollama server |

---

## üìù Notes

- Default model: `deepseek-v3.1:671b-cloud`
- Ollama API: `http://localhost:11434`
- Timeout: 5 minutes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generation
